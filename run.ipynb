{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from typing import Callable, Dict, List, Generator, Tuple\n",
    "from data_pre_process import *\n",
    "from model import *\n",
    "from data_loader import *\n",
    "from validation import *\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.cuda.amp \n",
    "from pathlib import Path\n",
    "from torch.cuda.amp import GradScaler as scaler\n",
    "\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizer, AdamW, BertModel, get_linear_schedule_with_warmup, BertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_start_time = time.time()\n",
    "\n",
    "bert_model = 'bert-base-uncased'\n",
    "do_lower_case = 'uncased' in bert_model\n",
    "device = torch.device('cuda')\n",
    "\n",
    "data_dir_t = Path('data_2/v1.0/train')\n",
    "data_path_t = data_dir_t/'nq-train-02.jsonl.gz'\n",
    "\n",
    "data_dir_v = Path('data_2/v1.0/dev')\n",
    "data_path_v = data_dir_v/'nq-dev-00.jsonl.gz'\n",
    "\n",
    "# data_dir_t = Path('data')\n",
    "# data_path_t = data_dir_t/'v1.0_train.jsonl.gz'\n",
    "\n",
    "# data_dir_v = Path('data')\n",
    "# data_path_v = data_dir_v/'v1.0_dev.jsonl.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyperparameters to convert a specific examples into multiple examples and the function that does it. \n",
    "Chunksize is the number of examples. Max sequence length is the size of a specific example will be broken down, and\n",
    "the overall content will be broken down in strides of 128.\n",
    "\"\"\"\n",
    "\n",
    "chunksize = 1000\n",
    "max_seq_len = 384\n",
    "max_question_len = 64\n",
    "doc_stride = 128\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case='uncased' in 'bert-base-uncased')\n",
    "\n",
    "convert_func = functools.partial(convert_data,\n",
    "                                 tokenizer=tokenizer,\n",
    "                                 max_seq_len=max_seq_len,\n",
    "                                 max_question_len=max_question_len,\n",
    "                                 doc_stride=doc_stride,\n",
    "                                 val=True) #this is to use document_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Opens a specific file and loads in all the examples which are per line, and calls jsonlreader which is an iterable \n",
    "used later on to train the model\n",
    "\"\"\"\n",
    "\n",
    "def open_file(data_path_t):\n",
    "    start = time.time()\n",
    "    with gzip.open(data_path_t, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    x = data.splitlines()\n",
    "    data_reader = JsonlReader(x, convert_func, chunksize=chunksize)\n",
    "    end = time.time()\n",
    "    train_size = len(x)\n",
    "    print(\"Loading Data:\", end - start, \"seconds\")\n",
    "    return data_reader, train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyperparameters that will be training my model on. Was only able to have a batchsize of 16 because low vram\n",
    "\"\"\"\n",
    "\n",
    "num_labels = 5\n",
    "n_epochs = 1\n",
    "lr = 2e-5\n",
    "warmup = 0.05\n",
    "batch_size = 16\n",
    "accumulation_steps = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initilization of model, paramters, optimizer and schedular\n",
    "\"\"\"\n",
    "desired_data_train_files = 6 #number of files you want to train on\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained(bert_model, num_labels=5)\n",
    "model = model.to(device)\n",
    "\n",
    "average_file_size = 6000\n",
    "train_size = average_file_size*desired_data_train_files\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "train_optimization_steps = int(n_epochs * train_size / batch_size / accumulation_steps)\n",
    "warmup_steps = int(train_optimization_steps * warmup)\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=lr, correct_bias=False)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=train_optimization_steps)\n",
    "\n",
    "model.zero_grad()\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training Loop that goes through multiple directories of files provided, then calls open_file, then iterates through\n",
    "each chunksize and loads the data which is broken to batches. The files loop through randomly, and the x_batch is a\n",
    "random batch of examples that were subsets of the same larger example.\n",
    "\"\"\"\n",
    "\n",
    "def train_per_file(data_reader, train_size):\n",
    "    running_loss = 0.0\n",
    "    global_step = 0\n",
    "    for examples in tqdm(data_reader, total=int(np.floor(train_size/chunksize))):\n",
    "        examples = list(filter(lambda example: len(example) != 0, examples))\n",
    "        train_dataset = TextDataset(examples)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, attention_mask, token_type_ids = x_batch\n",
    "            y_batch = (y.to(device) for y in y_batch)\n",
    "\n",
    "            y_pred = model(x_batch.to(device),\n",
    "                           attention_mask=attention_mask.to(device),\n",
    "                           token_type_ids=token_type_ids.to(device))\n",
    "\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if (global_step + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                model.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "    torch.save(model.state_dict(), 'bert_pytorch_all_t.bin')\n",
    "    torch.save(optimizer.state_dict(), 'bert_pytorch_optimizer_all_t.bin')\n",
    "    \n",
    "    del examples, train_dataset, train_loader\n",
    "    return running_loss/train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data_train_files = 0\n",
    "loss_per_file = []\n",
    "for child in data_dir_t.iterdir():\n",
    "    data_dir_t = child\n",
    "    data_reader, train_size = open_file(data_dir_t)\n",
    "    if train_size == 0:\n",
    "        continue\n",
    "    num_data_train_files += 1\n",
    "    loss = train_per_file(data_reader, train_size)\n",
    "    loss_per_file.append(loss)\n",
    "    if num_data_train_files >= desired_data_train_files:\n",
    "        break\n",
    "        \n",
    "print(loss_per_file)\n",
    "x = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plotting Loss Graph\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if len(loss_per_file) == 0:\n",
    "    loss_per_file = list(range(desired_data_train_files)) #so it will continue to next cell;\n",
    "\n",
    "plt.plot(list(range(desired_data_train_files)), loss_per_file)\n",
    "plt.xlabel(\"Number of Train Files\")\n",
    "plt.ylabel(\"Running Loss\")\n",
    "plt.title(\"Loss vs File\")\n",
    "plt.savefig(\"Loss.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loads a previous model if needed.\n",
    "\"\"\"\n",
    "load = False\n",
    "if load:\n",
    "    model.load_state_dict(torch.load(\"bert_pytorch.bin\"))\n",
    "    optimizer.load_state_dict(torch.load(\"bert_pytorch_optimizer.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reader_v, val_size = open_file(data_path_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_start_time = time.time()\n",
    "\n",
    "convert_func = functools.partial(convert_data,\n",
    "                                 tokenizer=tokenizer,\n",
    "                                 max_seq_len=max_seq_len,\n",
    "                                 max_question_len=max_question_len,\n",
    "                                 doc_stride=doc_stride,\n",
    "                                 val=True)\n",
    "\n",
    "valid_data = next(data_reader_v)\n",
    "valid_data = list(itertools.chain.from_iterable(valid_data))\n",
    "valid_dataset = Subset(valid_data, range(15000))\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=eval_collate_fn)\n",
    "valid_scores = eval_model(model, valid_loader, device=device)\n",
    "print(f'calculate validation score done in {(time.time() - eval_start_time) / 60:.1f} minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_score, long_p, long_recall = valid_scores['long_score']\n",
    "short_score, short_p, short_recall = valid_scores['short_score']\n",
    "overall_score = valid_scores['overall_score']\n",
    "print('validation scores:')\n",
    "print(f'\\tlong score    : {long_score:.4f}')\n",
    "print(f'\\tlong precision    : {long_p:.4f}')\n",
    "print(f'\\tlong_recall    : {long_recall:.4f}')\n",
    "print(f'\\tshort score   : {short_score:.4f}')\n",
    "print(f'\\toverall score : {overall_score:.4f}')\n",
    "print(f'all process done in {(time.time() - init_start_time) / 3600:.1f} hours.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = Subset(valid_data, range(len(valid_data)))\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=eval_collate_fn)\n",
    "valid_scores = eval_model(model, valid_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_score, long_p, long_recall = valid_scores['long_score']\n",
    "short_score, short_p, short_recall = valid_scores['short_score']\n",
    "overall_score = valid_scores['overall_score']\n",
    "print('validation scores:')\n",
    "print(f'\\tlong score    : {long_score:.4f}')\n",
    "print(f'\\tlong precision    : {long_p:.4f}')\n",
    "print(f'\\tlong_recall    : {long_recall:.4f}')\n",
    "print(f'\\tshort score   : {short_score:.4f}')\n",
    "print(f'\\toverall score : {overall_score:.4f}')\n",
    "print(f'all process done in {(time.time() - init_start_time) / 3600:.1f} hours.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
