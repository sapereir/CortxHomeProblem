{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from typing import Callable, Dict, List, Generator, Tuple\n",
    "from data_pre_process import *\n",
    "from model import *\n",
    "from data_loader import *\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.cuda.amp \n",
    "from pathlib import Path\n",
    "from torch.cuda.amp import GradScaler as scaler\n",
    "\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizer, AdamW, BertModel, get_linear_schedule_with_warmup, BertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_start_time = time.time()\n",
    "\n",
    "bert_model = 'bert-base-uncased'\n",
    "do_lower_case = 'uncased' in bert_model\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# data_dir_t = Path('data_2/v1.0/train')\n",
    "# data_path_t = data_dir_t/'nq-train-00.jsonl.gz'\n",
    "\n",
    "# data_dir_v = Path('data_2/v1.0/dev')\n",
    "# data_path_v = data_dir_v/'nq-dev-00.jsonl.gz'\n",
    "\n",
    "data_dir_t = Path('data')\n",
    "data_path_t = data_dir_t/'v1.0_train.jsonl.gz'\n",
    "\n",
    "data_dir_v = Path('data')\n",
    "data_path_v = data_dir_v/'v1.0_dev.jsonl.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 1000\n",
    "max_seq_len = 384\n",
    "max_question_len = 64\n",
    "doc_stride = 128\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case='uncased' in 'bert-base-uncased')\n",
    "\n",
    "convert_func = functools.partial(convert_data,\n",
    "                                 tokenizer=tokenizer,\n",
    "                                 max_seq_len=max_seq_len,\n",
    "                                 max_question_len=max_question_len,\n",
    "                                 doc_stride=doc_stride,\n",
    "                                 val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data: 153.53386163711548 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with gzip.open(data_path_t, \"rb\") as f:\n",
    "    data = f.read()\n",
    "x = data.splitlines()\n",
    "data_reader = JsonlReader(x, convert_func, chunksize=chunksize)\n",
    "end = time.time()\n",
    "print(\"Loading Data:\", end - start, \"seconds\")\n",
    "\n",
    "train_size = len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 5\n",
    "n_epochs = 1\n",
    "lr = 2e-5\n",
    "warmup = 0.05\n",
    "batch_size = 16\n",
    "accumulation_steps = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained(bert_model, num_labels=5)\n",
    "model = model.to(device)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "train_optimization_steps = int(n_epochs * train_size / batch_size / accumulation_steps)\n",
    "warmup_steps = int(train_optimization_steps * warmup)\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=lr, correct_bias=False)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=train_optimization_steps)\n",
    "\n",
    "s = torch.cuda.amp.GradScaler()\n",
    "model.zero_grad()\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "start_time = time.time()\n",
    "for examples in tqdm(data_reader, total=int(np.floor(train_size/chunksize))):\n",
    "    train_dataset = TextDataset(examples)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, attention_mask, token_type_ids = x_batch\n",
    "        y_batch = (y.to(device) for y in y_batch)\n",
    "        \n",
    "        y_pred = model(x_batch.to(device),\n",
    "                       attention_mask=attention_mask.to(device),\n",
    "                       token_type_ids=token_type_ids.to(device))\n",
    "        \n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        \n",
    "        if (global_step + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "        global_step += 1\n",
    "        \n",
    "    if int((time.time() - start_time) / 3600) % 4 == 0 and int((time.time() - start_time) / 3600) > 1:\n",
    "        torch.save(model.state_dict(), 'bert_pytorch.bin')\n",
    "        torch.save(optimizer.state_dict(), 'bert_pytorch_optimizer.bin')\n",
    "        break\n",
    "        \n",
    "del examples, train_dataset, train_loader\n",
    "x = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_collate_fn(examples: List[Example]) -> Tuple[List[torch.Tensor], List[Example]]:\n",
    "    # input tokens\n",
    "    max_len = max([len(example.input_ids) for example in examples])\n",
    "    tokens = np.zeros((len(examples), max_len), dtype=np.int64)\n",
    "    token_type_ids = np.ones((len(examples), max_len), dtype=np.int64)\n",
    "    for i, example in enumerate(examples):\n",
    "        row = example.input_ids\n",
    "        tokens[i, :len(row)] = row\n",
    "        token_type_id = [0 if i <= row.index(102) else 1\n",
    "                         for i in range(len(row))]  # 102 corresponds to [SEP]\n",
    "        token_type_ids[i, :len(row)] = token_type_id\n",
    "    attention_mask = tokens > 0\n",
    "    inputs = [torch.from_numpy(tokens),\n",
    "              torch.from_numpy(attention_mask),\n",
    "              torch.from_numpy(token_type_ids)]\n",
    "\n",
    "    return inputs, examples\n",
    "\n",
    "\n",
    "def eval_model(\n",
    "    model: nn.Module,\n",
    "    valid_loader: DataLoader,\n",
    "    device: torch.device = torch.device('cuda')\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Compute validation score.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        Model for prediction.\n",
    "    valid_loader : DataLoader\n",
    "        Data loader of validation data.\n",
    "    device : torch.device, optional\n",
    "        Device for computation.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Scores of validation data.\n",
    "        `long_score`: score of long answers\n",
    "        `short_score`: score of short answers\n",
    "        `overall_score`: score of the competition metric\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        result = Result()\n",
    "        for inputs, examples in tqdm(valid_loader):\n",
    "            input_ids, attention_mask, token_type_ids = inputs\n",
    "            y_preds = model(input_ids.to(device),\n",
    "                            attention_mask.to(device),\n",
    "                            token_type_ids.to(device))\n",
    "            \n",
    "            start_preds, end_preds, class_preds = (p.detach().cpu() for p in y_preds)\n",
    "            start_logits, start_index = torch.max(start_preds, dim=1)\n",
    "            end_logits, end_index = torch.max(end_preds, dim=1)\n",
    "\n",
    "            # span logits minus the cls logits seems to be close to the best\n",
    "            cls_logits = start_preds[:, 0] + end_preds[:, 0]  # '[CLS]' logits\n",
    "            logits = start_logits + end_logits - cls_logits  # (batch_size,)\n",
    "            indices = torch.stack((start_index, end_index)).transpose(0, 1)  # (batch_size, 2)\n",
    "            result.update(examples, logits.numpy(), indices.numpy(), class_preds.numpy())\n",
    "\n",
    "    return result.score()\n",
    "\n",
    "\n",
    "class Result(object):\n",
    "    \"\"\"Stores results of all test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.examples = {}\n",
    "        self.results = {}\n",
    "        self.best_scores = defaultdict(float)\n",
    "        self.class_labels = ['LONG', 'NO', 'SHORT', 'UNKNOWN', 'YES']\n",
    "        \n",
    "    @staticmethod\n",
    "    def is_valid_index(example: Example, index: List[int]) -> bool:\n",
    "        \"\"\"Return whether valid index or not.\n",
    "        \"\"\"\n",
    "        start_index, end_index = index\n",
    "        if start_index > end_index:\n",
    "            return False\n",
    "        if start_index <= example.question_len + 2:\n",
    "            return False\n",
    "        return True\n",
    "        \n",
    "    def update(\n",
    "        self,\n",
    "        examples: List[Example],\n",
    "        logits: torch.Tensor,\n",
    "        indices: torch.Tensor,\n",
    "        class_preds: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"Update batch objects.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        examples : list of Example\n",
    "        logits : np.ndarray with shape (batch_size,)\n",
    "            Scores of each examples..\n",
    "        indices : np.ndarray with shape (batch_size, 2)\n",
    "            `start_index` and `end_index` pairs of each examples.\n",
    "        class_preds : np.ndarray with shape (batch_size, num_classes)\n",
    "            Class predicition scores of each examples.\n",
    "        \"\"\"\n",
    "        for i, example in enumerate(examples):\n",
    "            if self.is_valid_index(example, indices[i]) and \\\n",
    "               self.best_scores[example.example_id] < logits[i]:\n",
    "                self.best_scores[example.example_id] = logits[i]\n",
    "                self.examples[example.example_id] = example\n",
    "                self.results[example.example_id] = [\n",
    "                    example.doc_start, indices[i], class_preds[i]]\n",
    "\n",
    "    def _generate_predictions(self) -> Generator[Dict, None, None]:\n",
    "        \"\"\"Generate predictions of each examples.\n",
    "        \"\"\"\n",
    "        for example_id in self.results.keys():\n",
    "            doc_start, index, class_pred = self.results[example_id]\n",
    "            example = self.examples[example_id]\n",
    "            tokenized_to_original_index = example.tokenized_to_original_index\n",
    "            short_start_index = tokenized_to_original_index[doc_start + index[0]]\n",
    "            short_end_index = tokenized_to_original_index[doc_start + index[1]]\n",
    "            long_start_index = -1\n",
    "            long_end_index = -1\n",
    "            for candidate in example.candidates:\n",
    "                if candidate['start_token'] <= short_start_index and \\\n",
    "                   short_end_index <= candidate['end_token']:\n",
    "                    long_start_index = candidate['start_token']\n",
    "                    long_end_index = candidate['end_token']\n",
    "                    break\n",
    "            yield {\n",
    "                'example': example,\n",
    "                'long_answer': [long_start_index, long_end_index],\n",
    "                'short_answer': [short_start_index, short_end_index],\n",
    "                'yes_no_answer': class_pred\n",
    "            }\n",
    "\n",
    "    def end(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Get predictions in submission format.\n",
    "        \"\"\"\n",
    "        preds = {}\n",
    "        for pred in self._generate_predictions():\n",
    "            example = pred['example']\n",
    "            long_start_index, long_end_index = pred['long_answer']\n",
    "            short_start_index, short_end_index = pred['short_answer']\n",
    "            class_pred = pred['yes_no_answer']\n",
    "\n",
    "            long_answer = f'{long_start_index}:{long_end_index}' if long_start_index != -1 else np.nan\n",
    "            short_answer = f'{short_start_index}:{short_end_index}'\n",
    "            class_pred = self.class_labels[class_pred.argmax()]\n",
    "            short_answer += ' ' + class_pred if class_pred in ['YES', 'NO'] else ''\n",
    "            preds[f'{example.example_id}_long'] = long_answer\n",
    "            preds[f'{example.example_id}_short'] = short_answer\n",
    "        return preds\n",
    "\n",
    "    def score(self) -> Dict[str, float]:\n",
    "        \"\"\"Calculate score of all examples.\n",
    "        \"\"\"\n",
    "\n",
    "        def _safe_divide(x: int, y: int) -> float:\n",
    "            \"\"\"Compute x / y, but return 0 if y is zero.\n",
    "            \"\"\"\n",
    "            if y == 0:\n",
    "                return 0.\n",
    "            else:\n",
    "                return x / y\n",
    "\n",
    "        def _compute_f1(answer_stats: List[List[bool]]) -> float:\n",
    "            \"\"\"Computes F1, precision, recall for a list of answer scores.\n",
    "            \"\"\"\n",
    "            has_answer, has_pred, is_correct = list(zip(*answer_stats))\n",
    "            precision = _safe_divide(sum(is_correct), sum(has_pred))\n",
    "            recall = _safe_divide(sum(is_correct), sum(has_answer))\n",
    "            f1 = _safe_divide(2 * precision * recall, precision + recall)\n",
    "            return f1\n",
    "\n",
    "        long_scores = []\n",
    "        short_scores = []\n",
    "        for pred in self._generate_predictions():\n",
    "            example = pred['example']\n",
    "            long_pred = pred['long_answer']\n",
    "            short_pred = pred['short_answer']\n",
    "            class_pred = pred['yes_no_answer']\n",
    "            yes_no_label = self.class_labels[class_pred.argmax()]\n",
    "\n",
    "            # long score\n",
    "            long_label = example.annotations['long_answer']\n",
    "            has_answer = long_label['candidate_index'] != -1\n",
    "            has_pred = long_pred[0] != -1 and long_pred[1] != -1\n",
    "            is_correct = False\n",
    "            if long_label['start_token'] == long_pred[0] and \\\n",
    "               long_label['end_token'] == long_pred[1]:\n",
    "                is_correct = True\n",
    "            long_scores.append([has_answer, has_pred, is_correct])\n",
    "\n",
    "            # short score\n",
    "            short_labels = example.annotations['short_answers']\n",
    "            class_pred = example.annotations['yes_no_answer']\n",
    "            has_answer = yes_no_label != 'NONE' or len(short_labels) != 0\n",
    "            has_pred = class_pred != 'NONE' or (short_pred[0] != -1 and short_pred[1] != -1)\n",
    "            is_correct = False\n",
    "            if class_pred in ['YES', 'NO']:\n",
    "                is_correct = yes_no_label == class_pred\n",
    "            else:\n",
    "                for short_label in short_labels:\n",
    "                    if short_label['start_token'] == short_pred[0] and \\\n",
    "                       short_label['end_token'] == short_pred[1]:\n",
    "                        is_correct = True\n",
    "                        break\n",
    "            short_scores.append([has_answer, has_pred, is_correct])\n",
    "\n",
    "        long_score = _compute_f1(long_scores)\n",
    "        short_score = _compute_f1(short_scores)\n",
    "        return {\n",
    "            'long_score': long_score,\n",
    "            'short_score': short_score,\n",
    "            'overall_score': (long_score + short_score) / 2\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data: 50.362544775009155 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with gzip.open(data_path_v, \"rb\") as f:\n",
    "    data = f.read()\n",
    "y = data.splitlines()\n",
    "end = time.time()\n",
    "print(\"Loading Data:\", end - start, \"seconds\")\n",
    "\n",
    "val_size = len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'document_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"/home/ubuntu/cortx/data_pre_process.py\", line 65, in convert_data\n    doc_words = data['document_text'].split()\nKeyError: 'document_text'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-247863d407a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0meval_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvalid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_reader_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvalid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cortx/data_pre_process.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         '''\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'document_text'"
     ]
    }
   ],
   "source": [
    "eval_start_time = time.time()\n",
    "\n",
    "convert_func = functools.partial(convert_data,\n",
    "                                 tokenizer=tokenizer,\n",
    "                                 max_seq_len=max_seq_len,\n",
    "                                 max_question_len=max_question_len,\n",
    "                                 doc_stride=doc_stride,\n",
    "                                 val=True)\n",
    "\n",
    "data_reader_v = JsonlReader(y, convert_func, chunksize=chunksize)\n",
    "valid_data = next(data_reader_v)\n",
    "valid_data = list(itertools.chain.from_iterable(valid_data))\n",
    "valid_dataset = Subset(valid_data, range(len(valid_data)))\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=eval_collate_fn)\n",
    "valid_scores = eval_model(model, valid_loader, device=device)\n",
    "\n",
    "print(f'calculate validation score done in {(time.time() - eval_start_time) / 60:.1f} minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_score = valid_scores['long_score']\n",
    "short_score = valid_scores['short_score']\n",
    "overall_score = valid_scores['overall_score']\n",
    "print('validation scores:')\n",
    "print(f'\\tlong score    : {long_score:.4f}')\n",
    "print(f'\\tshort score   : {short_score:.4f}')\n",
    "print(f'\\toverall score : {overall_score:.4f}')\n",
    "print(f'all process done in {(time.time() - init_start_time) / 3600:.1f} hours.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
