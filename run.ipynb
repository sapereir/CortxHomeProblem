{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from typing import Callable, Dict, List, Generator, Tuple\n",
    "from data_pre_process import *\n",
    "from model import *\n",
    "from data_loader import *\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.cuda.amp \n",
    "from pathlib import Path\n",
    "from torch.cuda.amp import GradScaler as scaler\n",
    "\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizer, AdamW, BertModel, get_linear_schedule_with_warmup, BertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = 'bert-base-uncased'\n",
    "do_lower_case = 'uncased' in bert_model\n",
    "device = torch.device('cuda')\n",
    "\n",
    "data_dir_t = Path('data_2/v1.0/train')\n",
    "data_path_t = data_dir_t/'nq-train-00.jsonl.gz'\n",
    "\n",
    "data_dir_v = Path('data_2/v1.0/dev')\n",
    "data_path_v = data_dir_v/'nq-dev-00.jsonl.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 10\n",
    "max_seq_len = 384\n",
    "max_question_len = 64\n",
    "doc_stride = 128\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case='uncased' in 'bert-base-uncased')\n",
    "\n",
    "convert_func = functools.partial(convert_data,\n",
    "                                 tokenizer=tokenizer,\n",
    "                                 max_seq_len=max_seq_len,\n",
    "                                 max_question_len=max_question_len,\n",
    "                                 doc_stride=doc_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data: 38.346622467041016 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with gzip.open(data_path_t, \"rb\") as f:\n",
    "    data = f.read()\n",
    "x = data.splitlines()\n",
    "data_reader = JsonlReader(x, convert_func, chunksize=chunksize)\n",
    "end = time.time()\n",
    "print(\"Loading Data:\", end - start, \"seconds\")\n",
    "\n",
    "train_size = len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 5\n",
    "n_epochs = 1\n",
    "lr = 2e-5\n",
    "warmup = 0.05\n",
    "batch_size = 16\n",
    "accumulation_steps = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained(bert_model, num_labels=5)\n",
    "model = model.to(device)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "train_optimization_steps = int(n_epochs * train_size / batch_size / accumulation_steps)\n",
    "warmup_steps = int(train_optimization_steps * warmup)\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=lr, correct_bias=False)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=train_optimization_steps)\n",
    "\n",
    "s = torch.cuda.amp.GradScaler()\n",
    "model.zero_grad()\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f3c88d67ff42f1b8437ad176055e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=597.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "global_step = 0\n",
    "for examples in tqdm(data_reader, total=int(np.ceil(train_size / chunksize))):\n",
    "    train_dataset = TextDataset(examples)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, attention_mask, token_type_ids = x_batch\n",
    "        y_batch = (y.to(device) for y in y_batch)\n",
    "\n",
    "        y_pred = model(x_batch.to(device),\n",
    "                       attention_mask=attention_mask.to(device),\n",
    "                       token_type_ids=token_type_ids.to(device))\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "#         with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        loss.backward()\n",
    "        if (global_step + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        global_step += 1\n",
    "        break\n",
    "        \n",
    "    break\n",
    "    if (time.time() - start_time) / 3600 > 7:\n",
    "        break\n",
    "\n",
    "del examples, train_dataset, train_loader\n",
    "x = gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
